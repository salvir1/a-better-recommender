{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "from lightfm import LightFM\n",
    "import scipy.stats as stats\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # for the pre-2.0 sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Always make it pretty.\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset-lightfm-no-features.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-50523a68dc3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpickle_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lightfm-no-features.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpersisted_light_FM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpickle_in_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset-lightfm-no-features.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpersisted_dataset_light_FM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_in_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset-lightfm-no-features.pkl'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import recommendation\n",
    "mPath =  \"als_recommender\"\n",
    "persisted_ALS_model = recommendation.ALSModel.load(mPath)\n",
    "\n",
    "pickle_in = open(\"lightfm-no-features.pkl\",\"rb\")\n",
    "persisted_light_FM = pickle.load(pickle_in)\n",
    "pickle_in_dataset = open(\"dataset-lightfm-no-features.pkl\",\"rb\")\n",
    "persisted_dataset_light_FM = pickle.load(pickle_in_dataset)\n",
    "\n",
    "validata = pd.read_csv('data/test_ratings_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prediction_scores(holdout_set):\n",
    "    count = 0\n",
    "    user_ground_truth, movie_mean_rating, lfm_rating, als_rating = [], [], [], []\n",
    "    ho_users = holdout_set.groupby('userId').count().reset_index()['userId']\n",
    "    for user in ho_users:\n",
    "        count += 1\n",
    "        if count < 4000:\n",
    "            sampled_user_movies = holdout_set[holdout_set['userId'] == user][['movieId','rating','mean']]#'movie Id numbers, not indices'\n",
    "            sampled_user_movies['userId'] = user\n",
    "\n",
    "            movie_samps = []\n",
    "            for movie in sampled_user_movies['movieId']:\n",
    "                movie_samps.append(persisted_dataset_light_FM.mapping()[2][movie])\n",
    "                \n",
    "            # ALS Model predictions\n",
    "            df2=spark.createDataFrame(sampled_user_movies)\n",
    "            ALS_pred=persisted_ALS_model.transform(df2).toPandas()\n",
    "#             print(ALS_pred.head(3))\n",
    "            \n",
    "            #LightFM model predictions\n",
    "            prediction = persisted_light_FM.predict(user_ids = persisted_dataset_light_FM.mapping()[0][user], item_ids = movie_samps, item_features=None, user_features=None)\n",
    "            sampled_user_movies['lfm_predict'] = prediction\n",
    "#             print(sampled_user_movies.sort_values(by='lfm_predict',ascending=False).head(3))\n",
    "            \n",
    "            sampled_user_movies=sampled_user_movies.drop(['rating', 'mean','userId'], axis=1)\n",
    "            sampled_user_movies=sampled_user_movies.merge(ALS_pred, how='inner', left_on='movieId', right_on='movieId')\n",
    "            sampled_user_movies=sampled_user_movies.rename(columns = {'prediction':'als_predict'})\n",
    "            \n",
    "            user_ground_truth.append(sampled_user_movies.sort_values('rating', ascending=False).head(5).sum()['rating'] / 5)\n",
    "            movie_mean_rating.append(sampled_user_movies.sort_values('mean', ascending=False).head(5).sum()['rating'] / 5)\n",
    "            lfm_rating.append(sampled_user_movies.sort_values('lfm_predict', ascending=False).head(5).sum()['rating'] / 5)\n",
    "            als_rating.append(sampled_user_movies.sort_values('als_predict', ascending=False).head(5).sum()['rating'] / 5)\n",
    "    \n",
    "    return user_ground_truth, movie_mean_rating, lfm_rating, als_rating, np.mean(user_ground_truth), np.std(user_ground_truth), np.mean(movie_mean_rating), np.std(movie_mean_rating), np.mean(lfm_rating), np.std(lfm_rating), np.mean(als_rating), np.std(als_rating)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground, pop_rtg, lfm_rtg, als_rtg, ground_mean, ground_std, pop_best, pop_std, lfm_best, lfm_std, als_best, als_std = calc_prediction_scores(validata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.556140350877193 3.9315789473684206 3.7561403508771924 4.06390977443609\n"
     ]
    }
   ],
   "source": [
    "print(ground_mean, pop_best, lfm_best, als_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_test = pd.DataFrame(ground, columns=(['ground']))\n",
    "bench_test['pop_rating'] = pd.DataFrame(pop_rtg)\n",
    "bench_test['lfm_rating'] = pd.DataFrame(lfm_rtg)\n",
    "bench_test['als_rating'] = pd.DataFrame(als_rtg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_test.to_csv('data/bench_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical comparisons\n",
    "- How do the results of the bench test compare? \n",
    "- Develop a hypothesis, and test the hypothesis.\n",
    "- Hypothesis: The mean score on the bench test data for the ALS model is 0.2 points higher than the score for the LFM model.\n",
    "- test statistic, p, will give us the probability of seeing observed results that are at least as extreme as what was measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_test_statistic(sample_1, sample_2):\n",
    "    numerator = np.mean(sample_1) - np.mean(sample_2)\n",
    "    denominator_sq = (np.var(sample_1) / len(sample_1)) + (np.var(sample_2) / len(sample_2))\n",
    "    return numerator / np.sqrt(denominator_sq)\n",
    "\n",
    "def welch_satterhwaithe_df(sample_1, sample_2):\n",
    "    ss1 = len(sample_1)\n",
    "    ss2 = len(sample_2)\n",
    "    df = (\n",
    "        ((np.var(sample_1)/ss1 + np.var(sample_2)/ss2)**(2.0)) / \n",
    "        ((np.var(sample_1)/ss1)**(2.0)/(ss1 - 1) + (np.var(sample_2)/ss2)**(2.0)/(ss2 - 1))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "bench_test = pd.read_csv('data/bench_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground</th>\n",
       "      <th>pop_rating</th>\n",
       "      <th>lfm_rating</th>\n",
       "      <th>als_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>399.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.556140</td>\n",
       "      <td>3.931579</td>\n",
       "      <td>3.756140</td>\n",
       "      <td>4.063910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.365821</td>\n",
       "      <td>0.571567</td>\n",
       "      <td>0.565869</td>\n",
       "      <td>0.486192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>4.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.900000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ground  pop_rating  lfm_rating  als_rating\n",
       "count  399.000000  399.000000  399.000000  399.000000\n",
       "mean     4.556140    3.931579    3.756140    4.063910\n",
       "std      0.365821    0.571567    0.565869    0.486192\n",
       "min      2.800000    1.200000    1.000000    1.800000\n",
       "25%      4.300000    3.700000    3.400000    3.800000\n",
       "50%      4.600000    4.000000    3.800000    4.100000\n",
       "75%      4.900000    4.300000    4.200000    4.400000\n",
       "max      5.000000    5.000000    5.000000    5.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statistic = welch_test_statistic(nyt_s_in_yes['CTR'], nyt_s_in_no['CTR'])\n",
    "print(\"Welch Test Statistic signed in vs not: {:2.2f}\".format(test_statistic))\n",
    "\n",
    "df = welch_satterhwaithe_df(nyt_s_in_yes['CTR'], nyt_s_in_no['CTR'])\n",
    "print(\"Degrees of Freedom for Welch's Test: {:2.2f}\".format(df))\n",
    "\n",
    "students = stats.t(df)\n",
    "p_value = students.cdf(test_statistic) + (1 - students.cdf(-test_statistic))\n",
    "print(\"p-value for signed in vs not different click-thru rate: {:2.2f}\".format(p_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
