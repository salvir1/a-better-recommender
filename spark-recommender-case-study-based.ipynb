{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, DateType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # for the pre-2.0 sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Training, Test and Validation datasets (ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------+---------+----------+----------+-------+---------+\n",
      "|userId|movieId|rating|count_x|     mean|       std|stat_score|count_y|   _merge|\n",
      "+------+-------+------+-------+---------+----------+----------+-------+---------+\n",
      "| 40630|    933|   4.0|   4827|3.9948208|0.76764643| 5.1462903|     43|left_only|\n",
      "| 40630|   1035|   4.5|  15248|3.8015149| 1.0843617|  5.428057|     43|left_only|\n",
      "| 40630|    922|   3.5|   7368| 4.206501| 0.8729187|  5.515879|     43|left_only|\n",
      "| 40630|    342|   2.0|  10735|3.5076385| 1.0187078| 5.0357003|     43|left_only|\n",
      "| 40630|   2724|   2.0|   8448|2.8626895|   1.08866| 4.4956794|     43|left_only|\n",
      "+------+-------+------+-------+---------+----------+----------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- count_x: integer (nullable = true)\n",
      " |-- mean: float (nullable = true)\n",
      " |-- std: float (nullable = true)\n",
      " |-- stat_score: float (nullable = true)\n",
      " |-- count_y: integer (nullable = true)\n",
      " |-- _merge: string (nullable = true)\n",
      "\n",
      "ratings has 7206304 rows\n"
     ]
    }
   ],
   "source": [
    "ratings_schema = StructType( [\n",
    "    StructField('userId',IntegerType(),True),\n",
    "    StructField('movieId',IntegerType(),True),\n",
    "    StructField('rating',FloatType(),True),\n",
    "    StructField('count_x',IntegerType(),True),\n",
    "    StructField('mean',FloatType(),True),\n",
    "    StructField('std',FloatType(),True),\n",
    "    StructField('stat_score',FloatType(),True),\n",
    "    StructField('count_y',IntegerType(),True),\n",
    "    StructField('_merge',StringType(),True)\n",
    "] )\n",
    "\n",
    "# read training CSV\n",
    "train_data = spark.read.csv('data/train_ratings_df.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=False,\n",
    "                         schema=ratings_schema)\n",
    "\n",
    "train_data.show(5)\n",
    "train_data.printSchema()\n",
    "print(\"ratings has {} rows\".format(train_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------+---------+---------+----------+-------+------+\n",
      "|userId|movieId|rating|count_x|     mean|      std|stat_score|count_y|_merge|\n",
      "+------+-------+------+-------+---------+---------+----------+-------+------+\n",
      "|     3|    778|   5.0|  28702| 4.007665|0.8891922| 5.3414536|    223|  null|\n",
      "|     3|    924|   5.0|  28820|3.9813497|1.0409104| 5.5427155|    223|  null|\n",
      "|     3| 122882|   3.5|  13479|3.8550339|1.0573668|  5.441084|    223|  null|\n",
      "|     3| 158238|   3.0|   3362| 3.752082|0.8232905|  4.987018|    223|  null|\n",
      "|     3|  40815|   4.0|  18848|  3.76788|0.9804957| 5.2386236|    223|  null|\n",
      "+------+-------+------+-------+---------+---------+----------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- count_x: integer (nullable = true)\n",
      " |-- mean: float (nullable = true)\n",
      " |-- std: float (nullable = true)\n",
      " |-- stat_score: float (nullable = true)\n",
      " |-- count_y: integer (nullable = true)\n",
      " |-- _merge: string (nullable = true)\n",
      "\n",
      "ratings has 191820 rows\n"
     ]
    }
   ],
   "source": [
    "# read validata CSV\n",
    "validata = spark.read.csv('data/test_ratings_df.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=False,\n",
    "                         schema=ratings_schema)\n",
    "\n",
    "validata.show(5)\n",
    "validata.printSchema()\n",
    "print(\"ratings has {} rows\".format(validata.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: float, count_x: int, mean: float, std: float, stat_score: float, count_y: int, _merge: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training, testing = train_data.randomSplit(weights = [0.80, 0.20], seed = 51)\n",
    "training.cache()\n",
    "testing.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few things on that split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users in training: 104964\n",
      "users in testing: 104560\n",
      "users in both: 104560\n",
      "movies in training: 12594\n",
      "movies in testing: 12404\n",
      "movies in both: 12404\n"
     ]
    }
   ],
   "source": [
    "print(\"users in training: {}\".format(training.select('userId').distinct().count()))\n",
    "print(\"users in testing: {}\".format(testing.select('userId').distinct().count()))\n",
    "\n",
    "print(\"users in both: {}\".format(training.select('userId').distinct()\\\n",
    "                                         .join(testing.select('userId').distinct(),\n",
    "                                               'userId', 'inner')\\\n",
    "                                         .count()))\n",
    "\n",
    "print(\"movies in training: {}\".format(training.select('movieId').distinct().count()))\n",
    "print(\"movies in testing: {}\".format(testing.select('movieId').distinct().count()))\n",
    "\n",
    "print(\"movies in both: {}\".format(training.select('movieId').distinct()\\\n",
    "                                         .join(testing.select('movieId').distinct(),\n",
    "                                               'movieId', 'inner')\\\n",
    "                                         .count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and fit and `ALS` algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_als = ALS(userCol=\"userId\",\n",
    "          itemCol=\"movieId\",\n",
    "          ratingCol=\"rating\")\n",
    "\n",
    "default_als = cv_als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing -- tried this--didn't add improvement to results\n",
    "# mean_rating, sttdev_rating = training.select(mean(\"rating\"), stddev(\"rating\")).first()\n",
    "# max_rating,min_rating = training.select(max(\"rating\"), min(\"rating\")).first()\n",
    "\n",
    "# training=training.withColumn(\"rating_Normalized\", (col(\"rating\") - mean_rating) / sttdev_rating)\n",
    "# training=training.withColumn(\"rating_minmax\", (col(\"rating\") - min_rating) /(max_rating-min_rating))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "\n",
    "# Tried to do the full param search but not enough memory. Notes on individual searches are below.\n",
    "paramMapExplicit = ParamGridBuilder() \\\n",
    "                    .addGrid(cv_als.rank, [10]) \\\n",
    "                    .addGrid(cv_als.maxIter, [5]) \\\n",
    "                    .addGrid(cv_als.regParam, [0.1, 0.15, 0.2]) \\\n",
    "                    .addGrid(cv_als.alpha, [1, 2.0]) \\\n",
    "                    .build()\n",
    "\n",
    "# rank [5,10,20] were searched with little difference between them\n",
    "# maxIter [5,10,20] were searched with little difference between them\n",
    "# regParam [0.01, 0.1, 0.2, 1] searched. Big falloff on shoulders.\n",
    "# alpha [1,2] were searched with little difference between them\n",
    "\n",
    "evaluatorR = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n",
    "\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "CVALSExplicit = CrossValidator(estimator=cv_als,\n",
    "                            estimatorParamMaps=paramMapExplicit,\n",
    "                            evaluator=evaluatorR,\n",
    "                           numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVModelEXplicit = CVALSExplicit.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVM_predictions = CVModelEXplicit.transform(testing)\n",
    "CVM_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate this model using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(CVM_predictions.na.fill({'prediction':2.5}))\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "# Optimal results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(rank=20,\n",
    "          maxIter=20,\n",
    "          regParam=0.1,\n",
    "          alpha=2,\n",
    "          userCol=\"userId\",\n",
    "          itemCol=\"movieId\",\n",
    "          ratingCol=\"rating\")\n",
    "\n",
    "ALS_fit_model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------+---------+---------+----------+-------+---------+----------+\n",
      "|userId|movieId|rating|count_x|     mean|      std|stat_score|count_y|   _merge|prediction|\n",
      "+------+-------+------+-------+---------+---------+----------+-------+---------+----------+\n",
      "| 35969|    148|   2.0|    335|2.9089553|1.0417528| 4.4715843|     67|left_only| 3.0657353|\n",
      "|135438|    148|   2.0|    335|2.9089553|1.0417528| 4.4715843|     24|left_only|  3.084729|\n",
      "|138573|    148|   3.0|    335|2.9089553|1.0417528| 4.4715843|     82|left_only| 2.5053427|\n",
      "|100706|    148|   3.0|    335|2.9089553|1.0417528| 4.4715843|    118|left_only|  2.685523|\n",
      "| 65981|    148|   3.5|    335|2.9089553|1.0417528| 4.4715843|    334|left_only| 2.7310896|\n",
      "+------+-------+------+-------+---------+---------+----------+-------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = ALS_fit_model.transform(testing)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.801075911230678\n"
     ]
    }
   ],
   "source": [
    "# Evaluate this model using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions.na.fill({'prediction':2.5})) # na.fill really isn't needed b/c of how train/test datasets were formed\n",
    "\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model to specified path\n",
    "mPath =  \"als_recommender\"\n",
    "ALS_fit_model.write().overwrite().save(mPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickled model via pipeline api\n",
    "from pyspark.ml import recommendation\n",
    "mPath =  \"als_recommender\"\n",
    "persisted_ALS_model = recommendation.ALSModel.load(mPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validata =validata.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>count_x</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>stat_score</th>\n",
       "      <th>count_y</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>778</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28702</td>\n",
       "      <td>4.007665</td>\n",
       "      <td>0.889192</td>\n",
       "      <td>5.341454</td>\n",
       "      <td>223</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>924</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28820</td>\n",
       "      <td>3.981350</td>\n",
       "      <td>1.040910</td>\n",
       "      <td>5.542716</td>\n",
       "      <td>223</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>122882</td>\n",
       "      <td>3.5</td>\n",
       "      <td>13479</td>\n",
       "      <td>3.855034</td>\n",
       "      <td>1.057367</td>\n",
       "      <td>5.441084</td>\n",
       "      <td>223</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  count_x      mean       std  stat_score  count_y  \\\n",
       "0       3      778     5.0    28702  4.007665  0.889192    5.341454      223   \n",
       "1       3      924     5.0    28820  3.981350  1.040910    5.542716      223   \n",
       "2       3   122882     3.5    13479  3.855034  1.057367    5.441084      223   \n",
       "\n",
       "  _merge  \n",
       "0   None  \n",
       "1   None  \n",
       "2   None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>count_x</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>stat_score</th>\n",
       "      <th>count_y</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12266</th>\n",
       "      <td>10824</td>\n",
       "      <td>158966</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1860</td>\n",
       "      <td>3.937903</td>\n",
       "      <td>0.874933</td>\n",
       "      <td>5.250302</td>\n",
       "      <td>432</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating  count_x      mean       std  stat_score  \\\n",
       "12266   10824   158966     4.5     1860  3.937903  0.874933    5.250302   \n",
       "\n",
       "       count_y _merge  \n",
       "12266      432   None  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = validata.sample(1) # Redraw sample\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_user = sample['userId'].iloc[0] # userId, not index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_user_movies = validata[validata['userId'] == sampled_user][['movieId','rating','mean']]#'movie Id numbers, not indices'\n",
    "sampled_user_movies['userId'] = sampled_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIDEBAR: How does ALS transform work?\n",
    "- ALS transform just returns predictions from the fit method. \n",
    "- It doesn't use new ratings info to update predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark.createDataFrame(sampled_user_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+------+----------+\n",
      "|movieId|rating|             mean|userId|prediction|\n",
      "+-------+------+-----------------+------+----------+\n",
      "|   8340|   4.5|3.776634931564331| 10824|  3.957441|\n",
      "| 135567|   0.5|2.796950340270996| 10824| 2.4839792|\n",
      "|   2671|   3.0|3.458824872970581| 10824| 2.9929905|\n",
      "|    527|   4.0|4.247579097747803| 10824|  4.202229|\n",
      "|   1084|   5.0|3.872877597808838| 10824|   4.11564|\n",
      "+-------+------+-----------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_user_pred=persisted_ALS_model.transform(df2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_user_movies['rating'] = 1\n",
    "df2=spark.createDataFrame(sampled_user_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+------+----------+\n",
      "|movieId|rating|             mean|userId|prediction|\n",
      "+-------+------+-----------------+------+----------+\n",
      "|   8340|     1|3.776634931564331| 10824|  3.957441|\n",
      "| 135567|     1|2.796950340270996| 10824| 2.4839792|\n",
      "|   2671|     1|3.458824872970581| 10824| 2.9929905|\n",
      "|    527|     1|4.247579097747803| 10824|  4.202229|\n",
      "|   1084|     1|3.872877597808838| 10824|   4.11564|\n",
      "+-------+------+-----------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_user_pred=persisted_ALS_model.transform(df2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIDE BAR: How does this model compare to those models made on case study day? \n",
    "**Case study day best in class was 3.7.**\n",
    "- Our objective is to integrate the scoring function into a CrossValidation loop of Spark.\n",
    "- To do that, we create our own spark object based on `RegressionEvaluator` with an overcasted `evaluate()` method of our own. - This method just calls the exact same function used in `src/submit.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecoRegressionEvaluation(RegressionEvaluator):\n",
    "    \"\"\" copy/pasted from submit.py \"\"\"\n",
    "    @staticmethod\n",
    "    def _compute_casestudy_score(predictions, actual):\n",
    "        \"\"\"Look at 5% of most highly predicted movies for each user.\n",
    "        Return the average actual rating of those movies.\n",
    "        \"\"\"\n",
    "        df = pd.merge(predictions, actual, on=['userId','movieId']).fillna(1.0)\n",
    "        #df = pd.concat([predictions.fillna(1.0), actual.actualrating], axis=1)\n",
    "\n",
    "        # for each user\n",
    "        g = df.groupby('userId')\n",
    "\n",
    "        # detect the top_5 movies as predicted by your algorithm\n",
    "        top_5 = g.rating.transform(\n",
    "            lambda x: x >= x.quantile(.95)\n",
    "        )\n",
    "\n",
    "        # return the mean of the actual score on those\n",
    "        return df.actualrating[top_5==1].mean()\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        # experimental\n",
    "        print(\"evaluate based on: {}, {}\".format(self.getLabelCol(),\n",
    "                                                 self.getPredictionCol()))\n",
    "        \n",
    "        # create a pandas dataframe that corresponds to argument predictions\n",
    "        pd_pred = dataset.select('userId','movieId',self.getPredictionCol())\\\n",
    "                         .withColumnRenamed(self.getPredictionCol(),'rating')\\\n",
    "                         .toPandas()\n",
    "                \n",
    "        # create a pandas dataframe that corresponds to argument actual\n",
    "        pd_actual = dataset.select('userId','movieId',self.getLabelCol())\\\n",
    "                         .withColumnRenamed(self.getLabelCol(),'actualrating')\\\n",
    "                         .toPandas()\n",
    "        \n",
    "        # call the exact same function from submit.py\n",
    "        return(self._compute_casestudy_score(pd_pred, pd_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing that on that loop 1 ALS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate based on: rating, prediction\n",
      "4.240729331970215\n"
     ]
    }
   ],
   "source": [
    "rec_evaluator = RecoRegressionEvaluation(labelCol=\"rating\",\n",
    "                                         predictionCol=\"prediction\")\n",
    "\n",
    "rec_score = rec_evaluator.evaluate(predictions)\n",
    "\n",
    "print(rec_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. integrate into a `MovieRecommender`-like class\n",
    "\n",
    "Doing that here will let us integrate this loop more easily with the next loops. All we have learned in this loop is thus contained in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieRecommender_loop1():\n",
    "    def fit(self, training_df):\n",
    "        self.als = ALS(rank=100,\n",
    "              maxIter=10,\n",
    "              regParam=0.1,\n",
    "              userCol=\"user\",\n",
    "              itemCol=\"movie\",\n",
    "              ratingCol=\"rating\")\n",
    "\n",
    "        self.loop1_model = self.als.fit(training_df)\n",
    "\n",
    "    def transform(self, requests_df):\n",
    "        return(self.loop1_model.transform(requests_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 ms, sys: 48.2 ms, total: 74.4 ms\n",
      "Wall time: 1min 31s\n",
      "CPU times: user 421 µs, sys: 1.12 ms, total: 1.54 ms\n",
      "Wall time: 105 ms\n",
      "evaluate based on: rating, prediction\n",
      "3.53959567516\n"
     ]
    }
   ],
   "source": [
    "mr1 = MovieRecommender_loop1()\n",
    "\n",
    "%time mr1.fit(training)\n",
    "\n",
    "rec_evaluator_loop1 = RecoRegressionEvaluation(labelCol=\"rating\",\n",
    "                                         predictionCol=\"prediction\")\n",
    "\n",
    "%time predictions_loop1 = mr1.transform(testing)\n",
    "\n",
    "print(rec_evaluator_loop1.evaluate(predictions_loop1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. develop a function to create submission [`loop1.csv`]\n",
    "\n",
    "We'll use this function based on our loop1 model.\n",
    "\n",
    "_Note: In the final submission we would train this model on the whole dataset, not on training only._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def write_submission(submission_df, output_filepath):\n",
    "    with open(output_filepath, 'w') as submissionfile:\n",
    "        submissionfile.write(\"user,movie,rating\\n\")\n",
    "        for row in submission_df.collect():\n",
    "            submissionfile.write(\"{},{},{}\\n\".format(row['user'],\n",
    "                                                     row['movie'],\n",
    "                                                     row['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note: When run on the testing set, this one will obtain a score of 3.62**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "write_submission(mr1.transform(requests),\n",
    "                 '../submissions/loop1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loop 2 : complete `NaN`s with something\n",
    "\n",
    "**Loop roadmap**:\n",
    "1. create an average score for each movie\n",
    "2. use average to fill predicted `NaN`\n",
    "3. integrate averaging into a `MovieRecommender`-like class\n",
    "4. develop a pipeline between loop1 and loop2\n",
    "5. generate a new submission file\n",
    "\n",
    "Let's first try to figure out how many predictions are `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73743"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_loop1.select('prediction').filter(F.isnan('prediction')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. create an average score for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|movie|       avg_rating|\n",
      "+-----+-----------------+\n",
      "|  858|4.524121500893389|\n",
      "|  593|4.364773319437207|\n",
      "| 2384|3.217877094972067|\n",
      "| 1961|4.033648790746582|\n",
      "| 2019|4.582795698924731|\n",
      "+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_avgratings = training.select('movie','rating')\\\n",
    "                            .groupBy('movie')\\\n",
    "                            .agg(F.avg('rating'))\\\n",
    "                            .withColumnRenamed('avg(rating)','avg_rating')\n",
    "\n",
    "movies_avgratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. use average to fill predicted `NaN`\n",
    "\n",
    "Now let's use that to complete predictions when there's `NaN`.\n",
    "1. We rename `prediction` as `prediction_als`.\n",
    "2. We join with `avg_rating`\n",
    "3. We define `prediction` as `avg_rating` if `prediction_als` is `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+---------+--------------+------------------+------------------+\n",
      "|movie|user|rating|timestamp|prediction_als|        avg_rating|        prediction|\n",
      "+-----+----+------+---------+--------------+------------------+------------------+\n",
      "|  148| 673|   5.0|975620824|           NaN|2.7777777777777777|2.7777777777777777|\n",
      "|  148|1242|   3.0|974909976|     2.6576235|2.7777777777777777| 2.657623529434204|\n",
      "|  148|1069|   2.0|974945135|           NaN|2.7777777777777777|2.7777777777777777|\n",
      "|  148|1605|   2.0|974930221|     2.1516573|2.7777777777777777|2.1516573429107666|\n",
      "|  148|1150|   2.0|974875106|           NaN|2.7777777777777777|2.7777777777777777|\n",
      "+-----+----+------+---------+--------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_loop2 = \\\n",
    "    predictions_loop1.withColumnRenamed('prediction','prediction_als')\\\n",
    "                     .join(movies_avgratings, 'movie', 'left')\\\n",
    "                     .withColumn('prediction',\n",
    "                                 F.when(F.isnan('prediction_als'),\n",
    "                                        F.col('avg_rating'))\\\n",
    "                                       .otherwise(F.col('prediction_als')))\n",
    "\n",
    "predictions_loop2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how THAT performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate based on: rating, prediction\n",
      "4.30716103235\n"
     ]
    }
   ],
   "source": [
    "rec_evaluator_loop2 = RecoRegressionEvaluation(labelCol=\"rating\",\n",
    "                                         predictionCol=\"prediction\")\n",
    "\n",
    "rec_score_loop2 = rec_evaluator_loop2.evaluate(predictions_loop2)\n",
    "\n",
    "print(rec_score_loop2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. integrate averaging into a `MovieRecommender`-like class\n",
    "\n",
    "Let's put that recommendation into a distinct class, and work on the integration with `MovieRecommender_loop1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MovieRecommender_loop2():\n",
    "    def fit(self, training_df):\n",
    "        self.avg_ratings = training_df.select('movie','rating')\\\n",
    "                                      .groupBy('movie')\\\n",
    "                                      .agg(F.avg('rating'))\\\n",
    "                                      .withColumnRenamed('avg(rating)','avg_rating')\n",
    "\n",
    "    def transform(self, requests_df):\n",
    "        return(requests_df.join(self.avg_ratings, 'movie', 'left')\\\n",
    "                          .withColumnRenamed('avg_rating','prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.76 ms, sys: 1.77 ms, total: 6.53 ms\n",
      "Wall time: 42.2 ms\n",
      "CPU times: user 1.2 ms, sys: 224 µs, total: 1.42 ms\n",
      "Wall time: 23 ms\n",
      "evaluate based on: rating, prediction\n",
      "4.27444565613\n"
     ]
    }
   ],
   "source": [
    "mr2 = MovieRecommender_loop2()\n",
    "\n",
    "%time mr2.fit(training)\n",
    "\n",
    "rec_evaluator_loop2 = RecoRegressionEvaluation(labelCol=\"rating\",\n",
    "                                         predictionCol=\"prediction\")\n",
    "\n",
    "%time predictions_loop2 = mr2.transform(testing)\n",
    "\n",
    "print(rec_evaluator_loop2.evaluate(predictions_loop2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. develop a pipeline between loop1 and loop2\n",
    "\n",
    "Now let's build a class that would aggregate ratings from loop1 and loop2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MovieRecommender_agg_loop2():\n",
    "    def __init__(self):\n",
    "        self.mr1 = MovieRecommender_loop1()\n",
    "        self.mr2 = MovieRecommender_loop2()\n",
    "        \n",
    "    def fit(self, training_df):\n",
    "        self.mr1.fit(training_df)\n",
    "        self.mr2.fit(training_df)\n",
    "        \n",
    "    def transform(self, requests_df):\n",
    "        pred_loop1 = self.mr1.transform(requests_df)\\\n",
    "                        .withColumnRenamed('prediction','prediction_loop1')\n",
    "\n",
    "        pred_loop2 = self.mr2.transform(pred_loop1)\\\n",
    "                        .withColumnRenamed('prediction','prediction_loop2')\n",
    "\n",
    "        results_loop2 = pred_loop2.withColumn('prediction',\n",
    "                                      F.when(F.isnan('prediction_loop1'),\n",
    "                                             F.col('prediction_loop2'))\\\n",
    "                                             .otherwise(F.col('prediction_loop1')))\n",
    "\n",
    "        #results_loop2.show(5)\n",
    "        return(results_loop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 ms, sys: 36.3 ms, total: 59.5 ms\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "mr_agg2 = MovieRecommender_agg_loop2()\n",
    "\n",
    "%time mr_agg2.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.53 ms, sys: 2.2 ms, total: 8.72 ms\n",
      "Wall time: 207 ms\n",
      "evaluate based on: rating, prediction\n",
      "4.30716103235\n"
     ]
    }
   ],
   "source": [
    "rec_evaluator_loop2 = RecoRegressionEvaluation(labelCol=\"rating\",\n",
    "                                         predictionCol=\"prediction\")\n",
    "\n",
    "%time predictions_agg2 = mr_agg2.transform(testing)\n",
    "\n",
    "print(rec_evaluator_loop2.evaluate(predictions_agg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. generate a new submission file [`loop2.csv`]\n",
    "\n",
    "This time, we train on the whole `ratings.csv` dataset instead.\n",
    "\n",
    "_**Note: When run on the testing set, this one will obtain a score of 4.34**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.2 ms, sys: 17.9 ms, total: 45.1 ms\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "mr_agg2 = MovieRecommender_agg_loop2()\n",
    "\n",
    "%time mr_agg2.fit(ratings)\n",
    "\n",
    "rec_evaluator_loop2 = RecoRegressionEvaluation(labelCol=\"rating\",\n",
    "                                         predictionCol=\"prediction\")\n",
    "\n",
    "write_submission(mr_agg2.transform(requests).na.fill({'prediction':1.0}),\n",
    "                 '../submissions/loop2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
